{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3431319e-2e6b-43c1-a2f6-61ccba592c21",
   "metadata": {},
   "source": [
    "## Evaluating a vectorial function on CPU and GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d142183-b6bb-42e7-ba9d-35bf0f91dc0c",
   "metadata": {},
   "source": [
    "### CPU: plain and numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b92dd336-9997-4323-80e4-f285e9cc2db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "8.26 ms ± 36.5 μs per loop (mean ± std. dev. of 2 runs, 5 loops each)\n",
      "18.7 ms ± 65.8 μs per loop (mean ± std. dev. of 2 runs, 5 loops each)\n",
      "18.8 ms ± 276 μs per loop (mean ± std. dev. of 2 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numba import njit, jit\n",
    "\n",
    "# Python plain implementation w/ numba \n",
    "@njit\n",
    "def grade2_vector(x, y, a, b, c):\n",
    "    z = np.zeros(x.size)\n",
    "    for i in range(x.size):\n",
    "        z[i] = a*x[i]*x[i] + b*y[i] + c\n",
    "    return z\n",
    "\n",
    "# Numpy ufunc\n",
    "def grade2_ufunc(x, y, a, b, c):\n",
    "    return a*x**2 + b*y + c\n",
    "\n",
    "# size of the vectors\n",
    "size = 5_000_000\n",
    "\n",
    "# allocating and populating the vectors\n",
    "a_cpu = np.random.rand(size)\n",
    "b_cpu = np.random.rand(size)\n",
    "c_cpu = np.zeros(size)\n",
    "\n",
    "a = 3.5\n",
    "b = 2.8\n",
    "c = 10\n",
    "\n",
    "# Printing input values\n",
    "#print(a_cpu)\n",
    "#print(b_cpu)\n",
    "# Random function in Numpy always use float64\n",
    "print(a_cpu.dtype)\n",
    "\n",
    "c_cpu = grade2_vector(a_cpu, b_cpu, a, b, c)\n",
    "\n",
    "\n",
    "# Evaluating the time\n",
    "\n",
    "# Numba Python: huge improvement, better that numpy code\n",
    "%timeit -n 5 -r 2 grade2_vector(a_cpu, b_cpu, a, b, c)\n",
    "\n",
    "# w/ a numpy ufunc manually coded\n",
    "%timeit -n 5 -r 2 grade2_ufunc(a_cpu, b_cpu, a, b, c)\n",
    "\n",
    "# using the general numpy ufunc \n",
    "%timeit -n 5 -r 2 a*a_cpu**2 + b*b_cpu + c\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d105023-fe80-4d64-9e5e-89764cb0cf09",
   "metadata": {},
   "source": [
    "## 3.2 a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c97a8a23-c0cf-429b-a529-329cac86fdfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo con arrays copiados CPU→GPU: 304.3159484863281 ms\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "\n",
    "def grade2_ufunc_cupy(x, y, a, b, c):\n",
    "    return a*x**2 + b*y + c\n",
    "    \n",
    "def gpu_time(func, *args):\n",
    "    start = cp.cuda.Event()\n",
    "    end = cp.cuda.Event()\n",
    "\n",
    "    start.record()\n",
    "    out = func(*args)\n",
    "    end.record()\n",
    "\n",
    "    end.synchronize()\n",
    "    return out, cp.cuda.get_elapsed_time(start, end)  # ms\n",
    "\n",
    "size = 5_000_000\n",
    "\n",
    "# Datos en CPU\n",
    "a_cpu = np.random.rand(size)\n",
    "b_cpu = np.random.rand(size)\n",
    "\n",
    "a = 3.5\n",
    "b = 2.8\n",
    "c = 10\n",
    "\n",
    "# Copiar CPU → GPU\n",
    "a_gpu = cp.asarray(a_cpu)\n",
    "b_gpu = cp.asarray(b_cpu)\n",
    "\n",
    "# Medir tiempo\n",
    "_, t_copy = gpu_time(grade2_ufunc_cupy, a_gpu, b_gpu, a, b, c)\n",
    "\n",
    "print(\"Tiempo con arrays copiados CPU→GPU:\", t_copy, \"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b4b607c-735d-43b2-91fa-a020f1cb4a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo sin copia (arrays creados en GPU): 4.683775901794434 ms\n"
     ]
    }
   ],
   "source": [
    "# Crear arrays directamente en GPU\n",
    "a_gpu2 = cp.random.rand(size)\n",
    "b_gpu2 = cp.random.rand(size)\n",
    "\n",
    "# Medir tiempo\n",
    "_, t_nocopy = gpu_time(grade2_ufunc_cupy, a_gpu2, b_gpu2, a, b, c)\n",
    "\n",
    "print(\"Tiempo sin copia (arrays creados en GPU):\", t_nocopy, \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843f59c3-0ac1-478a-a781-161f6303652c",
   "metadata": {},
   "source": [
    "## 3.2 b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75da8307-5a5b-4b6a-91ae-6fa34e0a0429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo con copia automática CPU→GPU: 60.58553695678711 ms\n"
     ]
    }
   ],
   "source": [
    "#Creamos la ufunc con vectorize\n",
    "from numba import vectorize\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "\n",
    "@vectorize(['float64(float64, float64, float64, float64, float64)'], target='cuda')\n",
    "def grade2_numba_gpu(x, y, a, b, c):\n",
    "    return a*x*x + b*y + c\n",
    "#Para medir el tiempo\n",
    "def gpu_time(func, *args):\n",
    "    start = cp.cuda.Event()\n",
    "    end = cp.cuda.Event()\n",
    "\n",
    "    start.record()\n",
    "    out = func(*args)\n",
    "    end.record()\n",
    "\n",
    "    end.synchronize()\n",
    "    return out, cp.cuda.get_elapsed_time(start, end)  \n",
    "\n",
    "size = 5_000_000\n",
    "\n",
    "# Datos en CPU\n",
    "a_cpu = np.random.rand(size)\n",
    "b_cpu = np.random.rand(size)\n",
    "\n",
    "a = 3.5\n",
    "b = 2.8\n",
    "c = 10\n",
    "\n",
    "# Medir tiempo (incluye copia CPU→GPU)\n",
    "_, t_auto_copy = gpu_time(grade2_numba_gpu, a_cpu, b_cpu, a, b, c)\n",
    "\n",
    "print(\"Tiempo con copia automática CPU→GPU:\", t_auto_copy, \"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "794ea9eb-eaaa-4d6b-b7db-fd3ddb63a82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo sin copia (arrays ya en GPU): 4.039135932922363 ms\n"
     ]
    }
   ],
   "source": [
    "# Copia manualmente a GPU\n",
    "a_gpu = cp.asarray(a_cpu)\n",
    "b_gpu = cp.asarray(b_cpu)\n",
    "\n",
    "# Medir tiempo sin copia\n",
    "_, t_no_copy = gpu_time(grade2_numba_gpu, a_gpu, b_gpu, a, b, c)\n",
    "\n",
    "print(\"Tiempo sin copia (arrays ya en GPU):\", t_no_copy, \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f970e73-d036-4d64-a799-6c54fa67f166",
   "metadata": {},
   "source": [
    "## 3.2 c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d46b81-b82a-4969-938c-748487f31bc3",
   "metadata": {},
   "source": [
    "## Resultados iniciales\n",
    "### Numba Python: huge improvement, better that numpy code\n",
    "8.26 ms ± 36.5 μs per loop (mean ± std. dev. of 2 runs, 5 loops each)\n",
    "\n",
    "### w/ a numpy ufunc manually coded\n",
    "18.7 ms ± 65.8 μs per loop (mean ± std. dev. of 2 runs, 5 loops each)\n",
    "\n",
    "### using the general numpy ufunc \n",
    "18.8 ms ± 276 μs per loop (mean ± std. dev. of 2 runs, 5 loops each)\n",
    "\n",
    "## Empleando Cupy\n",
    "Tiempo con arrays copiados CPU→GPU: 304.3159484863281 ms\n",
    "Al emplear CuPy copiando los arrays de la CPU a la GPU tenemos un tiempo alto debido a que la transferencia de CPU a GPU es el proceso más lento. Por otra parte, si no realizamos la copia de los arrays y los creamos directamente en GPU obtenemos el siguiente resultado: Tiempo sin copia (arrays creados en GPU): 4.683775901794434 ms\n",
    "Un tiempo significativamente menor, dejando claro que CuPy tiene un rendimiento muy alto. Además solo medimos el tiempo que tarda el kernel CUDA en realizar la operación y evitamos el tiempo de copia anterior. \n",
    "\n",
    "## Empleando Numba con ufunc y vectorize\n",
    "Al emplear numba podemos crear ufuncs que se ejecutan en la GPU, pero que se compilan con CUDA. Si pasamos los arrays a la ufunc se realiza la copia CPU→GPU internamente, este paso es que toma más tiempo y nos da el siguiente resultado: Tiempo con copia automática CPU→GPU: 60.58553695678711 ms\n",
    "En cambio, si antes de llamar a la ufunc realizamos la copia y por tanto no cuantificamos ese tiempo de copia, solo tendremos en cuenta el tiempo de ejecicución del kernel, obteniendo:Tiempo sin copia (arrays ya en GPU): 4.039135932922363 ms\n",
    "\n",
    "Esto deja claro que la GPU permite una aceleración importante del proceso sabiendo que el paso más costoso sería la copia de datos desde la CPU, si obviamos ese paso el rendimiento de CuPy y ufunc con vectorize es similar. En ambos debemos minimizar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
